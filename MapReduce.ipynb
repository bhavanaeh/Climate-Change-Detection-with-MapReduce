{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OOwBBNo7rfcK"
      },
      "source": [
        "# MAPREDUCE PROGRAMMING\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Map-Reduce is a process which will be work in three steps, namely map, shuffle, and reduce. The mapper's job is to process the input data in map stage. In Hadoop file system, the input data is in sort of file and is collected from various weather sites. And the reducer will take the output from the Map as an input and combined that data into a set of tuples.\n",
        "\n",
        "\n",
        "> DRIVER OPERATION\n",
        "---\n",
        "The driver sets up the job, submits it, and waits for the process to complete. It is taken from a configuration file to specify the input or output directories. It can also accept script based on mapper and reducer without re-compilation.\n",
        "\n",
        "\n",
        "> MAPPER OPERATION\n",
        "---\n",
        "The mapping is a simple process in which certain matched variables will be sent to the reducer. It considers of mappers and thus acts like a distributed search capability and pulls (key, value) pairs of a file. The input file format reader of hadoop opens files, which starts to read file for (key, value) pairs. Once it determines (key, value) pair, it reads both key and values which passes to the mapper and the mapping operator is used to filter out (key, value) pairs which do not match the criteria. Since mapper is not a part of Hadoop which read data, the mapper is collecting data from input file format reader and input file format reader is modified to read sequenced files. This routine opens a file and performs a simple loop to read each (key, value) within file. The desired key if matched with the filter, then values are read into memory and passed to the mapper. If filter does not match, then values are skipped. In the mapper, null values are to filter for calculation of a place, * id is used as a key and combination of date and place is used as map .*\n",
        "\n",
        "\n",
        "> REDUCE OPERATION\n",
        "---\n",
        "The resulting (key, value) pairs which matched the criteria are analyzed and forwarded to the reducer, sequencing and completing the mapping process. Once a (key, value) object has created, a comparator is needed to order keys. If data is combined, a group comparator may also be needed. A partitioner must be created in order to handle partitioning data into groups of sorted keys. With all these components in place, Hadoop takes the (key, value) pairs which is created by using mappers and groups and sorts them in a specified way. Hadoop assumes that all values sharing a key will be sent to the same reducer and a single operation over a large data set will be employed on one reducer. This gives us result in number of output files. \n",
        "\n",
        "---\n",
        "\n",
        "# MAP REDUCE MODEL\n",
        "\n",
        "The Map-Reduce process which is executed for minimum and maximum operation on the National climatic data is given as follows : \n",
        "\n",
        "1. Weather data set files are inserted into sequence files on head node of Hadoop Distributed File System. \n",
        "\n",
        "2. The sequence files which are loaded into Hadoop file system are loaded with replica factor three. \n",
        "\n",
        "3. The job provides Map-Reduce operation which is submitted to head node to run. The head node schedules the job tracker on cluster jobs which is to be run. Hadoop distributes mappers to all data nodes which contains data to analyze.\n",
        "\n",
        "4. For reading the input format reader opens up each sequence file which passes all the (key, value) pairs to map function on each node.\n",
        "\n",
        "5. The mapper determines, if key matches the criteria for query. If mapper keeps (key, value) pairs for delivery to the reducer. If not, The pair is discarded. The keys, values in a file are read and also analyze by using the mapper.\n",
        "\n",
        "6. The (key, value) pairs will match query and are to sent to the reducer performing the average function on the sorted pairs to create final (key, value) pair results. \n",
        "\n",
        "7. The final output is stored, in a Hadoop Distributed File System as a sequence file.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}